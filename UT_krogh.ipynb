{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55b5662a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "import julian\n",
    "from numpy import linalg\n",
    "from scipy.linalg import sqrtm\n",
    "from scipy.interpolate import KroghInterpolator\n",
    "\n",
    "\n",
    "def parse_ephemeris_file(filename: str):\n",
    "    \n",
    "    with open(filename, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    headers = {}\n",
    "    for i in range(3):\n",
    "        line = lines[i].strip()\n",
    "        if ':' in line:\n",
    "            key, value = line.split(':', 1)\n",
    "            headers[key] = value.strip()\n",
    "        if ' ' in line:\n",
    "            parts = line.split()\n",
    "            for part in parts:\n",
    "                if ':' in part:\n",
    "                    key, value = part.split(':', 1)\n",
    "                    headers[key] = value.strip()\n",
    "    \n",
    "    if lines[3].strip() != 'UVW':\n",
    "        raise ValueError(\"Expected UVW frame specification\")\n",
    "    \n",
    "    timestamps: List[datetime] = []\n",
    "    positions: List[np.ndarray] = []\n",
    "    velocities: List[np.ndarray] = []\n",
    "    covariances_uwv: List[np.ndarray] = []\n",
    "    covariances_eme: List[np.ndarray] = []\n",
    "    \n",
    "    i = 4\n",
    "    while i < len(lines):\n",
    "        state_parts = lines[i].strip().split()\n",
    "        \n",
    "        # More precise timestamp parsing\n",
    "        timestamp_str = state_parts[0]\n",
    "        year = int(timestamp_str[:4])\n",
    "        doy = int(timestamp_str[4:7])\n",
    "        hour = int(timestamp_str[7:9])\n",
    "        minute = int(timestamp_str[9:11])\n",
    "        second = int(timestamp_str[11:13])\n",
    "        millisec = int(timestamp_str[14:17])\n",
    "        \n",
    "        timestamp = datetime(year, 1, 1).replace(hour=hour, minute=minute,\n",
    "                                               second=second, microsecond=millisec*1000) + \\\n",
    "                   timedelta(days=doy-1)\n",
    "        \n",
    "        # Use np.float64 for higher precision\n",
    "        pos = np.array([np.float64(x) for x in state_parts[1:4]])\n",
    "        vel = np.array([np.float64(x) for x in state_parts[4:7]])\n",
    "        \n",
    "        cov_values = []\n",
    "        for j in range(3):\n",
    "            cov_values.extend([np.float64(x) for x in lines[i+1+j].strip().split()])\n",
    "        \n",
    "        cov_matrix_uwv = np.zeros((6, 6), dtype=np.float64)\n",
    "        idx = 0\n",
    "        for row in range(6):\n",
    "            for col in range(row + 1):\n",
    "                cov_matrix_uwv[row, col] = cov_values[idx]\n",
    "                cov_matrix_uwv[col, row] = cov_values[idx]\n",
    "                idx += 1\n",
    "        \n",
    "        \n",
    "        timestamps.append(timestamp)\n",
    "        positions.append(pos)\n",
    "        velocities.append(vel)\n",
    "        covariances_uwv.append(cov_matrix_uwv)\n",
    "        \n",
    "        i += 4\n",
    "    \n",
    "    return {\n",
    "        'headers': headers,\n",
    "        'timestamps': np.array(timestamps),\n",
    "        'positions': np.array(positions, dtype=np.float64),\n",
    "        'velocities': np.array(velocities, dtype=np.float64),\n",
    "        'covariances': np.array(covariances_uwv, dtype=np.float64),\n",
    "    }\n",
    "\n",
    "\n",
    "def reconstruct_covariance_at_time(interpolated_points: np.ndarray):\n",
    "    \"\"\"\n",
    "    Reconstruct the covariance matrix with improved numerical stability.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Optimized Unscented Transform parameters\n",
    "    n = 6\n",
    "    alpha = np.float64(0.001)  # Reduced alpha for better numerical stability\n",
    "    beta = np.float64(2.0)     # Optimal for Gaussian\n",
    "    kappa = np.float64(3-n)    # Modified for better stability\n",
    "    lambda_param = alpha * alpha * (n + kappa) - n\n",
    "\n",
    "    # Precompute weights for efficiency and precision\n",
    "    w0_m = lambda_param / (n + lambda_param)\n",
    "    wn_m = np.float64(0.5) / (n + lambda_param)\n",
    "    w0_c = w0_m + (1 - alpha * alpha + beta)\n",
    "    wn_c = wn_m\n",
    "        \n",
    "    \n",
    "    # Calculate mean state with improved precision\n",
    "    mean_state = np.zeros(6, dtype=np.float64)\n",
    "    mean_state = interpolated_points[0]\n",
    "    #mean_state = w0_m * interpolated_points[0]\n",
    "    #for i in range(1, len(interpolated_points)):\n",
    "    #    mean_state += wn_m * interpolated_points[i]\n",
    "    \n",
    "    # Calculate covariance with improved numerical stability\n",
    "    covariance = np.zeros((6, 6), dtype=np.float64)\n",
    "    \n",
    "    diff_0 = interpolated_points[0] - mean_state\n",
    "    covariance = w0_c * np.outer(diff_0, diff_0)\n",
    "    \n",
    "    for i in range(1, len(interpolated_points)):\n",
    "        diff = interpolated_points[i] - mean_state\n",
    "        covariance += wn_c * np.outer(diff, diff)\n",
    "        \n",
    "    covariance = (covariance + covariance.T) / 2\n",
    "\n",
    "    return mean_state, covariance\n",
    "\n",
    "\n",
    "        \n",
    "def generate_and_propagate_sigma_points(data):\n",
    "    \"\"\"\n",
    "    Generate and propagate sigma points with improved numerical stability.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Use high precision for Julian date conversion\n",
    "        julian_dates = np.array([julian.to_jd(ts) for ts in data['timestamps']], dtype=np.float64)\n",
    "        \n",
    "        state_vectors = np.hstack((data['positions'], data['velocities']))\n",
    "        covariances = data['covariances']\n",
    "        \n",
    "        sigma_points_dict = {}\n",
    "        \n",
    "        # Optimized Unscented Transform parameters\n",
    "        n = 6\n",
    "        alpha = np.float64(0.001)  # Reduced alpha for better numerical stability\n",
    "        beta = np.float64(2.0)     # Optimal for Gaussian\n",
    "        kappa = np.float64(3-n)    # Modified for better stability\n",
    "        lambda_param = alpha * alpha * (n + kappa) - n\n",
    "        \n",
    "        # Precompute weights for efficiency and precision\n",
    "        w0_m = lambda_param / (n + lambda_param)\n",
    "        wn_m = np.float64(0.5) / (n + lambda_param)\n",
    "        w0_c = w0_m + (1 - alpha * alpha + beta)\n",
    "        wn_c = wn_m\n",
    "        \n",
    "        weights = {\n",
    "            'mean': {'w0': w0_m, 'wn': wn_m},\n",
    "            'covariance': {'w0': w0_c, 'wn': wn_c}\n",
    "        }\n",
    "        \n",
    "        for idx, (jd, timestamp) in enumerate(zip(julian_dates, data['timestamps'])):\n",
    "            try:\n",
    "                state_vector = state_vectors[idx]\n",
    "                covariance = covariances[idx]\n",
    "                \n",
    "                # Ensure symmetry of covariance matrix\n",
    "                covariance = (covariance + covariance.T) / 2\n",
    "                \n",
    "                # Scale covariance matrix with improved numerical stability\n",
    "                scaled_cov = (n+lambda_param) * covariance\n",
    "                \n",
    "                # Try Cholesky first, fall back to modified sqrtm if needed\n",
    "                try:\n",
    "                    L = np.linalg.cholesky(scaled_cov)\n",
    "                except np.linalg.LinAlgError:\n",
    "                    L = sqrtm(scaled_cov)\n",
    "                \n",
    "                sigma_0 = state_vector\n",
    "                sigma_n = sigma_0[:, np.newaxis] + L\n",
    "                sigma_2n = sigma_0[:, np.newaxis] - L\n",
    "                \n",
    "                all_sigma_points = np.vstack([\n",
    "                                    sigma_0,\n",
    "                                    sigma_n.T,\n",
    "                                    sigma_2n.T\n",
    "                                ])\n",
    "                \n",
    "                sigma_points_dict[jd] = {\n",
    "                    'sigma_points': all_sigma_points,\n",
    "                    'weights': weights,\n",
    "                    'epoch': timestamp,\n",
    "                    'state_vector': state_vector,\n",
    "                    'covariance': covariance\n",
    "                    }\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Failed to process timestamp {timestamp}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        if not sigma_points_dict:\n",
    "            raise ValueError(\"No sigma points were successfully generated\")\n",
    "        \n",
    "        return sigma_points_dict\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in generate_and_propagate_sigma_points: {str(e)}\")\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4038eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_chunked_krogh_interpolator(x: np.ndarray, y: np.ndarray, \n",
    "                                    chunk_size: int = 14, \n",
    "                                    overlap: int = 8):\n",
    "    \"\"\"\n",
    "    Create a chunked Krogh interpolation function to avoid numerical instability\n",
    "    when interpolating over many points.\n",
    "    \n",
    "    Args:\n",
    "        x: Independent variable values (e.g., times)\n",
    "        y: Dependent variable values to interpolate\n",
    "        chunk_size: Number of points to use in each interpolation chunk\n",
    "        overlap: Number of points to overlap between chunks\n",
    "        \n",
    "    Returns:\n",
    "        List of interpolators and their valid ranges\n",
    "    \"\"\"\n",
    "    if len(x) <= chunk_size:\n",
    "        # If data is smaller than chunk size, just use a single interpolator\n",
    "        interp = KroghInterpolator(x, y)\n",
    "        return [{'interpolator': interp, 'range': (x[0], x[-1])}]\n",
    "    \n",
    "    # Split data into overlapping chunks\n",
    "    interpolators = []\n",
    "    \n",
    "    i = 0\n",
    "    while i < len(x):\n",
    "        end_idx = min(i + chunk_size, len(x))\n",
    "        chunk_x = x[i:end_idx]\n",
    "        chunk_y = y[i:end_idx]\n",
    "        \n",
    "        # Create interpolator for this chunk\n",
    "        interp = KroghInterpolator(chunk_x, chunk_y)\n",
    "        \n",
    "        # Record the valid range for this chunk (slightly narrower than the actual chunk)\n",
    "        # to ensure smooth transitions between chunks\n",
    "        if i == 0:\n",
    "            # First chunk - use from beginning to just before end\n",
    "            valid_range = (chunk_x[0], chunk_x[-2] if len(chunk_x) > 2 else chunk_x[-1])\n",
    "        elif end_idx == len(x):\n",
    "            # Last chunk - use from just after start to end\n",
    "            valid_range = (chunk_x[1] if len(chunk_x) > 1 else chunk_x[0], chunk_x[-1])\n",
    "        else:\n",
    "            # Middle chunks - use middle portion, leaving overlap areas for adjacent chunks\n",
    "            valid_range = (chunk_x[1] if len(chunk_x) > 1 else chunk_x[0], \n",
    "                          chunk_x[-2] if len(chunk_x) > 2 else chunk_x[-1])\n",
    "        \n",
    "        interpolators.append({'interpolator': interp, 'range': valid_range})\n",
    "        \n",
    "        # Move to next chunk with overlap, ensuring we make progress\n",
    "        i = max(end_idx - overlap, i + 1)\n",
    "    \n",
    "    return interpolators\n",
    "    \n",
    "    # Create a function that selects the appropriate chunk and interpolates\n",
    "\n",
    "def interpolate_sigma_pointsKI(sigma_points_dict):\n",
    "    \"\"\"\n",
    "    Create high-precision interpolation splines for sigma point trajectories.\n",
    "    \"\"\"\n",
    "    # Get and sort Julian dates with high precision\n",
    "    julian_dates = np.array(sorted(sigma_points_dict.keys()), dtype=np.float64)\n",
    "    \n",
    "    n_sigma_points = 13\n",
    "    \n",
    "    # Initialize with explicit types\n",
    "    positions_by_point = [[] for _ in range(n_sigma_points)]\n",
    "    velocities_by_point = [[] for _ in range(n_sigma_points)]\n",
    "    \n",
    "    # Extract position and velocity components with high precision\n",
    "    for jd in julian_dates:\n",
    "        sigma_points = sigma_points_dict[jd]['sigma_points'].astype(np.float64)\n",
    "        for i in range(n_sigma_points):\n",
    "            positions_by_point[i].append(sigma_points[i][:3])\n",
    "            velocities_by_point[i].append(sigma_points[i][3:])\n",
    "    \n",
    "    # Convert to numpy arrays with high precision\n",
    "    positions_by_point = [np.array(pos, dtype=np.float64) for pos in positions_by_point]\n",
    "    velocities_by_point = [np.array(vel, dtype=np.float64) for vel in velocities_by_point]\n",
    "    \n",
    "    position_splines = []\n",
    "    velocity_splines = []\n",
    "    \n",
    "    \n",
    "    for i in range(n_sigma_points):\n",
    "        # Position splines\n",
    "        pos_splines_i = []\n",
    "        for j in range(3):\n",
    "            pos_data = positions_by_point[i][:, j]\n",
    "            valid_mask = np.isfinite(pos_data)\n",
    "            if np.any(valid_mask):\n",
    "                # Use not-a-knot cubic splines for better accuracy\n",
    "                spline = create_chunked_krogh_interpolator(julian_dates[valid_mask], \n",
    "                                    pos_data[valid_mask],\n",
    "                                   chunk_size= 14, \n",
    "                                    overlap= 8)\n",
    "                pos_splines_i.append(spline)\n",
    "            else:\n",
    "                pos_splines_i.append(None)\n",
    "        position_splines.append(pos_splines_i)\n",
    "        \n",
    "        # Velocity splines\n",
    "        vel_splines_i = []\n",
    "        for j in range(3):\n",
    "            vel_data = velocities_by_point[i][:, j]\n",
    "            valid_mask = np.isfinite(vel_data)\n",
    "            if np.any(valid_mask):\n",
    "                spline = create_chunked_krogh_interpolator(julian_dates[valid_mask], \n",
    "                                    vel_data[valid_mask],\n",
    "                                   chunk_size= 14, \n",
    "                                    overlap= 8)\n",
    "                vel_splines_i.append(spline)\n",
    "            else:\n",
    "                vel_splines_i.append(None)\n",
    "        velocity_splines.append(vel_splines_i)\n",
    "    \n",
    "    return {\n",
    "        'positions': position_splines,\n",
    "        'velocities': velocity_splines,\n",
    "        'time_range': (julian_dates[0], julian_dates[-1])\n",
    "    }\n",
    "\n",
    "\n",
    "def get_interpolated_sigma_points_KI(interpolated_splines,\n",
    "                                  julian_date):\n",
    "    \"\"\"\n",
    "    Get interpolated sigma points with high precision using Krogh interpolation.\n",
    "    \n",
    "    Selects the optimal interpolator when in overlap regions - preferring \n",
    "    interpolators where the point is in the middle of their range rather\n",
    "    than at the edges.\n",
    "    \"\"\"\n",
    "    start_time, end_time = interpolated_splines['time_range']\n",
    "    if not (start_time <= julian_date <= end_time):\n",
    "        raise ValueError(f\"Requested time {julian_date} is outside the interpolation range \"\n",
    "                        f\"[{start_time}, {end_time}]\")\n",
    "    \n",
    "    n_sigma_points = 13\n",
    "    interpolated_points = np.zeros((n_sigma_points, 6), dtype=np.float64)\n",
    "    \n",
    "    for i in range(n_sigma_points):\n",
    "        # Interpolate positions with high precision\n",
    "        for j in range(3):\n",
    "            if interpolated_splines['positions'][i][j] is not None:\n",
    "                splines = interpolated_splines['positions'][i][j]\n",
    "                \n",
    "                # Find all applicable splines for this time\n",
    "                applicable_splines = []\n",
    "                for idx, spline_info in enumerate(splines):\n",
    "                    lower, upper = spline_info['range']\n",
    "                    if lower <= julian_date <= upper:\n",
    "                        # Calculate how central the point is within this spline's range\n",
    "                        # (0.5 means it's in the middle, 0 or 1 means it's at an edge)\n",
    "                        centrality = (julian_date - lower) / (upper - lower) \n",
    "                        # Prefer points that are more central (closer to 0.5)\n",
    "                        score = 1 - abs(centrality - 0.5) * 2  # Converts to 0-1 scale where 1 is most central\n",
    "                        applicable_splines.append((idx, score, spline_info))\n",
    "                \n",
    "                # If we found applicable splines, use the most central one\n",
    "                if applicable_splines:\n",
    "                    # Sort by score (descending)\n",
    "                    applicable_splines.sort(key=lambda x: x[1], reverse=True)\n",
    "                    best_spline = applicable_splines[0][2]\n",
    "                    interpolated_points[i, j] = best_spline['interpolator'](julian_date)\n",
    "                else:\n",
    "                    # If no spline's range contains this point, use the closest one\n",
    "                    if julian_date < start_time:\n",
    "                        interpolated_points[i, j] = splines[0]['interpolator'](julian_date)\n",
    "                    else:\n",
    "                        interpolated_points[i, j] = splines[-1]['interpolator'](julian_date)\n",
    "        \n",
    "        # Interpolate velocities with high precision - using the same approach\n",
    "        for j in range(3):\n",
    "            if interpolated_splines['velocities'][i][j] is not None:\n",
    "                splines = interpolated_splines['velocities'][i][j]\n",
    "                \n",
    "                # Find all applicable splines for this time\n",
    "                applicable_splines = []\n",
    "                for idx, spline_info in enumerate(splines):\n",
    "                    lower, upper = spline_info['range']\n",
    "                    if lower <= julian_date <= upper:\n",
    "                        # Calculate how central the point is within this spline's range\n",
    "                        centrality = (julian_date - lower) / (upper - lower) \n",
    "                        # Prefer points that are more central\n",
    "                        score = 1 - abs(centrality - 0.5) * 2\n",
    "                        applicable_splines.append((idx, score, spline_info))\n",
    "                \n",
    "                # If we found applicable splines, use the most central one\n",
    "                if applicable_splines:\n",
    "                    applicable_splines.sort(key=lambda x: x[1], reverse=True)\n",
    "                    best_spline = applicable_splines[0][2]\n",
    "                    interpolated_points[i, j+3] = best_spline['interpolator'](julian_date)\n",
    "                else:\n",
    "                    # If no spline's range contains this point, use the closest one\n",
    "                    if julian_date < start_time:\n",
    "                        interpolated_points[i, j+3] = splines[0]['interpolator'](julian_date)\n",
    "                    else:\n",
    "                        interpolated_points[i, j+3] = splines[-1]['interpolator'](julian_date)\n",
    "    \n",
    "    return interpolated_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be543a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing ephemeris file: SpaceX/MEME_47375_STARLINK-2094_0371242_Operational_1423140180_UNCLASSIFIED.txt\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    filename = 'SpaceX/MEME_47375_STARLINK-2094_0371242_Operational_1423140180_UNCLASSIFIED.txt'\n",
    "    \n",
    "    # Parse ephemeris file with validation\n",
    "    print(f\"Parsing ephemeris file: {filename}\")\n",
    "    data = parse_ephemeris_file(filename)\n",
    "    if data is None:\n",
    "        raise ValueError(\"Failed to parse ephemeris file\")  \n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error in main: {str(e)}\")\n",
    "    raise\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e364bc7a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Generate sigma points with validation\n",
    "sigma_points_dict = generate_and_propagate_sigma_points(data)\n",
    "if sigma_points_dict is None:\n",
    "    raise ValueError(\"Failed to generate sigma points\")\n",
    "\n",
    "# Create interpolation splines\n",
    "interpolated_splinesKI = interpolate_sigma_pointsKI(sigma_points_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "351023a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#just to get JDs\n",
    "julian_dates = np.array([julian.to_jd(ts) for ts in data['timestamps']], dtype=np.float64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd971e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5.36212576e-07 -3.89360380e-07  2.24578081e-10  9.22153844e-10\n",
      "  -4.70590643e-10  8.41867405e-13]\n",
      " [-3.89360380e-07  7.83264118e-07  6.12165133e-12 -9.04725435e-10\n",
      "   4.12498662e-10 -3.22523119e-13]\n",
      " [ 2.24578081e-10  6.12165133e-12  1.26446529e-06 -9.82477462e-15\n",
      "   1.60555565e-13  1.70171911e-09]\n",
      " [ 9.22153844e-10 -9.04725435e-10 -9.82477462e-15  2.09318813e-12\n",
      "  -8.26358766e-13  6.18798637e-16]\n",
      " [-4.70590643e-10  4.12498662e-10  1.60555565e-13 -8.26358766e-13\n",
      "   5.02552868e-13 -2.21761651e-16]\n",
      " [ 8.41867405e-13 -3.22523119e-13  1.70171911e-09  6.18798637e-16\n",
      "  -2.21761651e-16  5.42035153e-12]]\n"
     ]
    }
   ],
   "source": [
    "#boundary of data \n",
    "jd_start=julian_dates[0]\n",
    "jd_end=julian_dates[-1]\n",
    "\n",
    "#any values between jd_start and jd_end\n",
    "jd=julian_dates[0]+1/86400\n",
    "\n",
    "# METHOD 2: Krogh interpolation (using sigma points)\n",
    "interpolated_points = get_interpolated_sigma_points_KI(interpolated_splinesKI,jd )\n",
    "interp_mean, interp_cov = reconstruct_covariance_at_time(interpolated_points)\n",
    "print(interp_cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b5adea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5.35222150e-07 -3.88257092e-07  2.23386181e-10  9.20279894e-10\n",
      "  -4.69679110e-10  8.42965307e-13]\n",
      " [-3.88257092e-07  7.81584930e-07  6.88366829e-12 -9.02407155e-10\n",
      "   4.11433007e-10 -3.23976066e-13]\n",
      " [ 2.23386181e-10  6.88366829e-12  1.26106628e-06 -9.95123677e-15\n",
      "   1.58979792e-13  1.69781253e-09]\n",
      " [ 9.20279894e-10 -9.02407155e-10 -9.95123677e-15  2.08934260e-12\n",
      "  -8.24578970e-13  6.23262868e-16]\n",
      " [-4.69679110e-10  4.11433007e-10  1.58979792e-13 -8.24578970e-13\n",
      "   5.01737831e-13 -2.26327602e-16]\n",
      " [ 8.42965307e-13 -3.23976066e-13  1.69781253e-09  6.23262868e-16\n",
      "  -2.26327602e-16  5.42395019e-12]]\n"
     ]
    }
   ],
   "source": [
    "#boundary of data \n",
    "jd_start=julian_dates[0]\n",
    "jd_end=julian_dates[-1]\n",
    "\n",
    "#any values between jd_start and jd_end\n",
    "jd=julian_dates[0]\n",
    "\n",
    "# METHOD 2: Krogh interpolation (using sigma points)\n",
    "interpolated_points = get_interpolated_sigma_points_KI(interpolated_splinesKI,jd )\n",
    "interp_mean, interp_cov = reconstruct_covariance_at_time(interpolated_points)\n",
    "print(interp_cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b86e54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
